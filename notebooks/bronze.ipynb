{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f36f306",
   "metadata": {},
   "source": [
    "# **NOAA GHCN Project Using Spark Medallion Architecture**\n",
    "\n",
    "## **Project Overview**\n",
    "\n",
    "* Collect daily climate observations from the **NOAA GHCN-Daily dataset (2010–2025)**\n",
    "* Dataset source: [Global Historical Climatology Network – Daily (GHCN-D)](https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-daily)\n",
    "* Process the data through the **Bronze → Silver → Gold** Medallion Architecture using Spark\n",
    "* Perform exploratory analysis to study **historical climate trends** in temperature and precipitation\n",
    "* Use machine-learning methods to identify and predict:\n",
    "\n",
    "  * Temperature anomalies\n",
    "  * Rainfall patterns\n",
    "  * Regional climate clusters\n",
    "  * Potential extreme events (e.g., heatwaves)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3afd4a8",
   "metadata": {},
   "source": [
    "# **Bronze Layer**\n",
    "\n",
    "In the Bronze layer we ingest the raw GHCN-Daily text file and convert it into a structured, append-only dataset using Spark.\n",
    "\n",
    "**Key steps:**\n",
    "- Download selected years from the dataset (One Time Only)\n",
    "- Read the combined raw file as a text DataFrame (one line per record)\n",
    "- Parse each line safely into a structured schema\n",
    "- Use a safe parser that:\n",
    "  - labels well-formed records as `_status = \"valid\"`\n",
    "  - routes malformed lines to `_status = \"parse_error\"` and keeps the original raw line in `_raw_data`\n",
    "- Write the Bronze dataset as Parquet, partitioned by `year`, to the configured `BRONZE_OUT` path.\n",
    "- Inspect the Bronze output by printing the schema, showing sample rows, and listing distinct `element` values present in the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6764f965",
   "metadata": {},
   "source": [
    "## 01. Download GHCN Data and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1de753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download GHCN files to a local folder and combines them into a text file using the helper script.\n",
    "#import os, subprocess\n",
    "#\n",
    "#years = list(range(2010, 2026))  # e.g., 2010..2025\n",
    "#target_dir = os.environ.get(\"NOAA_DIR\", \"/home/ubuntu/spark-notebooks/project/data/raw\")\n",
    "#os.makedirs(target_dir, exist_ok=True)\n",
    "#\n",
    "#env = os.environ.copy()\n",
    "#env[\"DATA_DIR\"] = target_dir\n",
    "#\n",
    "#try:\n",
    "#    # Use the helper script located at project root\n",
    "#    cmd = [\"bash\", \"/home/ubuntu/spark-notebooks/project/scripts/ghcn_download.sh\", *[str(y) for y in years]]\n",
    "#    print(\"Running:\", cmd)\n",
    "#    subprocess.run(cmd, check=True, env=env)\n",
    "#except Exception as e:\n",
    "#    print(\"Error downloading GHCN data:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd224682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download GHCN metadata files (stations & inventory)\n",
    "# import os, subprocess\n",
    "\n",
    "# # Define where metadata will be stored\n",
    "# meta_dir = \"/home/ubuntu/spark-notebooks/project/data/meta\"\n",
    "# os.makedirs(meta_dir, exist_ok=True)\n",
    "\n",
    "# # URLs for metadata files\n",
    "# urls = {\n",
    "#     \"ghcnd-stations.txt\": \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt\",\n",
    "#     \"ghcnd-inventory.txt\": \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-inventory.txt\"\n",
    "# }\n",
    "\n",
    "# for filename, url in urls.items():\n",
    "#     dest_path = os.path.join(meta_dir, filename)\n",
    "#     if os.path.exists(dest_path) and os.path.getsize(dest_path) > 0:\n",
    "#         print(f\"[skip] {filename} already exists at {dest_path}\")\n",
    "#         continue\n",
    "#     try:\n",
    "#         print(f\"[download] {filename} from {url}\")\n",
    "#         subprocess.run([\"wget\", \"-O\", dest_path, url], check=True)\n",
    "#         print(f\"[done] Saved to {dest_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"[error] Could not download {filename}: {e}\")\n",
    "\n",
    "# # Show downloaded files\n",
    "# print(\"\\nMetadata files:\")\n",
    "# !ls -lh /home/ubuntu/spark-notebooks/project/data/meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18e1f2e",
   "metadata": {},
   "source": [
    "## 02. Spark Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "740856ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-vm:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>NOAA-GHCN-Bronze</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7df2b8401ed0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Spark session\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"NOAA-GHCN-Bronze\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"Spark version:\", spark.version)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae7a828",
   "metadata": {},
   "source": [
    "## 03. Data Path Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c48606d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dir: /home/ubuntu/spark-notebooks/project/data/raw\n",
      "Bronze out: /home/ubuntu/spark-notebooks/project/data/bronze\n"
     ]
    }
   ],
   "source": [
    "# Define input/output paths\n",
    "import os\n",
    "\n",
    "# Local folder with raw .txt files\n",
    "RAW_DIR = os.environ.get(\"NOAA_DIR\", \"/home/ubuntu/spark-notebooks/project/data/raw\")\n",
    "\n",
    "# Bronze Parquet outputs\n",
    "BRONZE_OUT = os.environ.get(\"BRONZE_OUT\", \"/home/ubuntu/spark-notebooks/project/data/bronze\")\n",
    "\n",
    "print(\"Input dir:\", RAW_DIR)\n",
    "print(\"Bronze out:\", BRONZE_OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53fedf7",
   "metadata": {},
   "source": [
    "## 04. Read raw .txt file as a Spark text DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4b9e818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from: /home/ubuntu/spark-notebooks/project/data/raw/ghcn_all_years.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw line count: 585758447\n",
      "+--------------------------------+\n",
      "|value                           |\n",
      "+--------------------------------+\n",
      "|ASN00010195,20100101,PRCP,0,,,a,|\n",
      "|ASN00010160,20100101,PRCP,0,,,a,|\n",
      "|ASN00010163,20100101,PRCP,0,,,a,|\n",
      "+--------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "\n",
    "# Strictly use the combined TXT file\n",
    "input_path = os.path.join(RAW_DIR, \"ghcn_all_years.txt\")\n",
    "if not os.path.isfile(input_path):\n",
    "    raise FileNotFoundError(f\"Expected combined TXT file at {input_path}. Run the download cell to generate it.\")\n",
    "\n",
    "df_text = spark.read.text(input_path)\n",
    "print(\"Reading from:\", input_path)\n",
    "print(\"Raw line count:\", df_text.count())\n",
    "df_text.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54aafaf",
   "metadata": {},
   "source": [
    "## 05. Safe Parsing of Raw Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "242d376e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze schema:\n",
      "root\n",
      " |-- station: string (nullable = true)\n",
      " |-- date_str: string (nullable = true)\n",
      " |-- element: string (nullable = true)\n",
      " |-- raw_value: string (nullable = true)\n",
      " |-- mflag: string (nullable = true)\n",
      " |-- qflag: string (nullable = true)\n",
      " |-- sflag: string (nullable = true)\n",
      " |-- obstime: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- _ingestion_timestamp: double (nullable = false)\n",
      " |-- _source: string (nullable = false)\n",
      " |-- _status: string (nullable = false)\n",
      " |-- _raw_data: string (nullable = true)\n",
      "\n",
      "Bronze sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+---------+-----+-----+-----+-------+----+--------------------+--------+-------+---------+\n",
      "|station    |date_str|element|raw_value|mflag|qflag|sflag|obstime|year|_ingestion_timestamp|_source |_status|_raw_data|\n",
      "+-----------+--------+-------+---------+-----+-----+-----+-------+----+--------------------+--------+-------+---------+\n",
      "|ASN00010195|20100101|PRCP   |0        |NULL |NULL |a    |NULL   |2010|1.7640004042068837E9|ghcn_txt|valid  |NULL     |\n",
      "|ASN00010160|20100101|PRCP   |0        |NULL |NULL |a    |NULL   |2010|1.764000404206972E9 |ghcn_txt|valid  |NULL     |\n",
      "|ASN00010163|20100101|PRCP   |0        |NULL |NULL |a    |NULL   |2010|1.764000404206985E9 |ghcn_txt|valid  |NULL     |\n",
      "|ASN00010192|20100101|PRCP   |0        |NULL |NULL |a    |NULL   |2010|1.7640004042070355E9|ghcn_txt|valid  |NULL     |\n",
      "|ASN00010111|20100101|TMAX   |330      |NULL |NULL |a    |NULL   |2010|1.7640004042070467E9|ghcn_txt|valid  |NULL     |\n",
      "+-----------+--------+-------+---------+-----+-----+-----+-------+----+--------------------+--------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:======================================================>(155 + 2) / 157]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze rows: 585758447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Bronze-safe parsing: read raw lines and convert them into structured records\n",
    "\n",
    "import time\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "# Schema for the Bronze layer: raw fields + ingestion metadata\n",
    "bronze_schema = StructType([\n",
    "    StructField(\"station\", StringType(), True),\n",
    "    StructField(\"date_str\", StringType(), True),\n",
    "    StructField(\"element\", StringType(), True),\n",
    "    StructField(\"raw_value\", StringType(), True),\n",
    "    StructField(\"mflag\", StringType(), True),\n",
    "    StructField(\"qflag\", StringType(), True),\n",
    "    StructField(\"sflag\", StringType(), True),\n",
    "    StructField(\"obstime\", StringType(), True),\n",
    "    StructField(\"year\", StringType(), True),                    # extracted year for partitioning\n",
    "    StructField(\"_ingestion_timestamp\", DoubleType(), False),\n",
    "    StructField(\"_source\", StringType(), False),\n",
    "    StructField(\"_status\", StringType(), False),\n",
    "    StructField(\"_raw_data\", StringType(), True),\n",
    "])\n",
    "\n",
    "SOURCE_TAG = \"ghcn_txt\"\n",
    "\n",
    "# Safe parser that captures all fields or records a parse error\n",
    "def parse_line_safe(line: str):\n",
    "    ts = time.time()\n",
    "    try:\n",
    "        parts = line.split(\",\")\n",
    "        if len(parts) < 8:\n",
    "            raise ValueError(\"not enough columns\")\n",
    "        station = parts[0] or None\n",
    "        date_str = parts[1] or None\n",
    "        element = parts[2] or None\n",
    "        raw_value = parts[3] or None\n",
    "        mflag = parts[4] or None\n",
    "        qflag = parts[5] or None\n",
    "        sflag = parts[6] or None\n",
    "        obstime = parts[7] or None\n",
    "        year = (date_str[:4] if date_str and len(date_str) >= 4 else None)\n",
    "        return Row(\n",
    "            station=station,\n",
    "            date_str=date_str,\n",
    "            element=element,\n",
    "            raw_value=raw_value,\n",
    "            mflag=mflag,\n",
    "            qflag=qflag,\n",
    "            sflag=sflag,\n",
    "            obstime=obstime,\n",
    "            year=year,\n",
    "            _ingestion_timestamp=ts,\n",
    "            _source=SOURCE_TAG,\n",
    "            _status=\"valid\",\n",
    "            _raw_data=None,\n",
    "        )\n",
    "    except Exception:\n",
    "        # Preserve the full raw line when parsing fails\n",
    "        return Row(\n",
    "            station=None,\n",
    "            date_str=None,\n",
    "            element=None,\n",
    "            raw_value=None,\n",
    "            mflag=None,\n",
    "            qflag=None,\n",
    "            sflag=None,\n",
    "            obstime=None,\n",
    "            year=None,\n",
    "            _ingestion_timestamp=ts,\n",
    "            _source=SOURCE_TAG,\n",
    "            _status=\"parse_error\",\n",
    "            _raw_data=line,\n",
    "        )\n",
    "\n",
    "# Convert text input into RDD and apply safe parser\n",
    "rdd = df_text.rdd.map(lambda r: parse_line_safe(r[\"value\"]))\n",
    "\n",
    "# Build the Bronze DataFrame from parsed rows\n",
    "df_bronze = spark.createDataFrame(rdd, schema=bronze_schema)\n",
    "\n",
    "print(\"Bronze schema:\")\n",
    "df_bronze.printSchema()\n",
    "\n",
    "print(\"Bronze sample:\")\n",
    "df_bronze.show(5, truncate=False)\n",
    "\n",
    "print(\"Bronze rows:\", df_bronze.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff82d7ca",
   "metadata": {},
   "source": [
    "## 06. Write Bronze as Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ef909eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 23:07:07 WARN MemoryManager: Total allocation exceeds 95.00% (964,270,478 bytes) of heap memory\n",
      "Scaling row group sizes to 89.80% for 8 writers\n",
      "25/11/11 23:08:33 WARN MemoryManager: Total allocation exceeds 95.00% (964,270,478 bytes) of heap memory\n",
      "Scaling row group sizes to 89.80% for 8 writers\n",
      "25/11/11 23:08:45 WARN MemoryManager: Total allocation exceeds 95.00% (964,270,478 bytes) of heap memory\n",
      "Scaling row group sizes to 89.80% for 8 writers\n",
      "25/11/11 23:09:40 WARN MemoryManager: Total allocation exceeds 95.00% (964,270,478 bytes) of heap memory\n",
      "Scaling row group sizes to 89.80% for 8 writers\n",
      "[Stage 8:======================================================>(156 + 1) / 157]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze Parquet written to: /home/ubuntu/spark-notebooks/data/bronze\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write Bronze as Parquet, partitioned by year\n",
    "df_bronze.write.mode(\"overwrite\").partitionBy(\"year\").parquet(BRONZE_OUT)\n",
    "print(\"Bronze Parquet written to:\", BRONZE_OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ef37ba",
   "metadata": {},
   "source": [
    "## 07. Inspect elements present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef20bbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct element count: 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:=====================================================>(156 + 1) / 157]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|element|\n",
      "+-------+\n",
      "|ADPT   |\n",
      "|ASLP   |\n",
      "|ASTP   |\n",
      "|AWBT   |\n",
      "|AWDR   |\n",
      "|AWND   |\n",
      "|DAEV   |\n",
      "|DAPR   |\n",
      "|DASF   |\n",
      "|DATN   |\n",
      "|DATX   |\n",
      "|DAWM   |\n",
      "|DWPR   |\n",
      "|EVAP   |\n",
      "|FMTM   |\n",
      "|MDEV   |\n",
      "|MDPR   |\n",
      "|MDSF   |\n",
      "|MDTN   |\n",
      "|MDTX   |\n",
      "|MDWM   |\n",
      "|MNPN   |\n",
      "|MXPN   |\n",
      "|PGTM   |\n",
      "|PRCP   |\n",
      "|PSUN   |\n",
      "|RHAV   |\n",
      "|RHMN   |\n",
      "|RHMX   |\n",
      "|SN02   |\n",
      "|SN03   |\n",
      "|SN11   |\n",
      "|SN12   |\n",
      "|SN13   |\n",
      "|SN14   |\n",
      "|SN21   |\n",
      "|SN22   |\n",
      "|SN23   |\n",
      "|SN31   |\n",
      "|SN32   |\n",
      "|SN33   |\n",
      "|SN34   |\n",
      "|SN35   |\n",
      "|SN36   |\n",
      "|SN51   |\n",
      "|SN52   |\n",
      "|SN53   |\n",
      "|SN54   |\n",
      "|SN55   |\n",
      "|SN56   |\n",
      "+-------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "elements = df_bronze.select(\"element\").distinct().orderBy(\"element\")\n",
    "print(\"Distinct element count:\", elements.count())\n",
    "elements.show(50, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
