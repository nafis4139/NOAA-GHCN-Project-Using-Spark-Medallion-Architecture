{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6c3bf4c",
   "metadata": {},
   "source": [
    "# **Silver Layer**\n",
    "\n",
    "In the Silver layer we transform the raw Bronze records into cleaned, daily station-level climate data and structured metadata tables.\n",
    "\n",
    "**Key steps:**\n",
    "- Read all Bronze Parquet data and restrict to years `2010–2025`.\n",
    "- Switch to RDD-based processing and map each record to a simpler structure\n",
    "- Apply quality filtering:\n",
    "  - keep only records with empty quality flag (`qflag == \"\"`);\n",
    "  - keep only valid/empty ingestion status (`status in [\"\", \"valid\"]`);\n",
    "  - drop missing/sentinel values (`value is not None` and `value != -9999`);\n",
    "  - keep only the three core elements: **TMAX**, **TMIN**, **PRCP**.\n",
    "- Deduplicate by `(id, date, element)` and convert raw units to SI:\n",
    "  - temperatures to °C (`tmax_c`, `tmin_c`, derived `tavg_c`),\n",
    "  - precipitation to millimetres (`prcp_mm`).\n",
    "- Aggregate to one row per station–day `(id, date, year)` with:\n",
    "  - `tmax_c`, `tmin_c`, `tavg_c`, `prcp_mm`.\n",
    "- Create a Silver DataFrame, parse the date, and add convenience fields:\n",
    "  - `date` (as `yyyy-MM-dd`), `month`, `day`, `year`.\n",
    "- Write the Silver fact table as Parquet, partitioned by `year`, to `SILVER_PATH`\n",
    "\n",
    "**MetaData:**\n",
    "- Parse `ghcnd-stations.txt` (fixed-width) into a cleaned **stations** table\n",
    "- Parse `ghcnd-inventory.txt` (fixed-width) into an **inventory** table\n",
    "- Derive a small **coverage** helper table summarizing the min/max years\n",
    "  of TMAX/TMIN/PRCP coverage per station.\n",
    "- Store all metadata as Parquet under `silver_meta`\n",
    "  (`stations.parquet`, `inventory.parquet`, `coverage.parquet`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e0caff",
   "metadata": {},
   "source": [
    "## 01. Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "852f43a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/26 19:51:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"GHCN-Silver-RDD\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7267dc1c",
   "metadata": {},
   "source": [
    "## 02. Data Paths and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9430a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "BRONZE_PATH = \"/home/ubuntu/spark-notebooks/project/data/bronze\" \n",
    "SILVER_PATH = \"/home/ubuntu/spark-notebooks/project/data/silver\" \n",
    "\n",
    "# Year bounds for faster iteration\n",
    "YEAR_MIN = 2010\n",
    "YEAR_MAX = 2025\n",
    "\n",
    "# Elements to keep in first pass\n",
    "ELEMENTS = {\"TMAX\", \"TMIN\", \"PRCP\"}\n",
    "\n",
    "VERBOSE_SAMPLES = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a3ac1b",
   "metadata": {},
   "source": [
    "## 03. Read Bronze and switch to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f408c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['station', 'date_str', 'element', 'raw_value', 'mflag', 'qflag', 'sflag', 'obstime', '_ingestion_timestamp', '_source', '_status', '_raw_data', 'year']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|year|\n",
      "+----+\n",
      "|2010|\n",
      "|2011|\n",
      "|2012|\n",
      "|2013|\n",
      "|2014|\n",
      "|2015|\n",
      "|2016|\n",
      "|2017|\n",
      "|2018|\n",
      "|2019|\n",
      "|2020|\n",
      "|2021|\n",
      "|2022|\n",
      "|2023|\n",
      "|2024|\n",
      "|2025|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking bronze data\n",
    "df_bronze = spark.read.parquet(\"/home/ubuntu/spark-notebooks/project/data/bronze\")\n",
    "print(\"Columns:\", df_bronze.columns)\n",
    "df_bronze.select(\"year\").distinct().orderBy(\"year\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aa65515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze columns: ['station', 'date_str', 'element', 'raw_value', 'mflag', 'qflag', 'sflag', 'obstime', '_ingestion_timestamp', '_source', '_status', '_raw_data', 'year']\n",
      "Bronze count: 587797263\n",
      "root\n",
      " |-- station: string (nullable = true)\n",
      " |-- date_str: string (nullable = true)\n",
      " |-- element: string (nullable = true)\n",
      " |-- raw_value: string (nullable = true)\n",
      " |-- mflag: string (nullable = true)\n",
      " |-- qflag: string (nullable = true)\n",
      " |-- sflag: string (nullable = true)\n",
      " |-- obstime: string (nullable = true)\n",
      " |-- _ingestion_timestamp: double (nullable = true)\n",
      " |-- _source: string (nullable = true)\n",
      " |-- _status: string (nullable = true)\n",
      " |-- _raw_data: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Read Bronze (partitioned by year). We filter by year on the DF read,\n",
    "# but we won't use DF operations for cleaning itself.\n",
    "df_bronze = spark.read.format(\"parquet\").load(BRONZE_PATH)\n",
    "\n",
    "# Using 'year' column to prune the read for faster iteration.\n",
    "if \"year\" in df_bronze.columns:\n",
    "    df_bronze = df_bronze.where((F.col(\"year\") >= YEAR_MIN) & (F.col(\"year\") <= YEAR_MAX))\n",
    "\n",
    "print(\"Bronze columns:\", df_bronze.columns)\n",
    "print(\"Bronze count:\", df_bronze.count())\n",
    "df_bronze.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7ee2731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First raw record: {'station': 'CA001063303', 'date_str': '20180917', 'element': 'TMAX', 'raw_value': '146', 'mflag': None, 'qflag': None, 'sflag': 'C', 'obstime': None, '_ingestion_timestamp': 1764178290.6534057, '_source': 'ghcn_txt', '_status': 'valid', '_raw_data': None, 'year': 2018}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Switch to RDD for all cleaning\n",
    "rdd_raw = df_bronze.rdd\n",
    "print(\"First raw record:\", rdd_raw.first().asDict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1837a7",
   "metadata": {},
   "source": [
    "## 04. RDD Cleaning (MapReduce style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69d304f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows (bounded): 587797263\n",
      "Elements present (pre-clean): [('ADPT', 2484948), ('ASLP', 2480218), ('ASTP', 2480218), ('AWBT', 2484948), ('AWDR', 141537), ('AWND', 6237038), ('DAEV', 7), ('DAPR', 1578216), ('DASF', 314), ('DATN', 42373), ('DATX', 40999), ('DAWM', 2), ('DWPR', 125145), ('EVAP', 770845), ('FMTM', 333968), ('MDEV', 5), ('MDPR', 1580047), ('MDSF', 1662), ('MDTN', 42372), ('MDTX', 40999), ('MDWM', 2), ('MNPN', 362678), ('MXPN', 365989), ('PGTM', 2014083), ('PRCP', 170899310), ('PSUN', 18956), ('RHAV', 2496957), ('RHMN', 2498781), ('RHMX', 2498781), ('SN02', 3389), ('SN03', 6), ('SN11', 351), ('SN12', 6693), ('SN13', 1087), ('SN14', 396), ('SN21', 30), ('SN22', 30), ('SN23', 30), ('SN31', 104928), ('SN32', 844656), ('SN33', 78544), ('SN34', 1173), ('SN35', 21705), ('SN36', 9729), ('SN51', 32024), ('SN52', 341801), ('SN53', 42189), ('SN54', 982), ('SN55', 16818), ('SN56', 6628), ('SN57', 31), ('SNOW', 73934574), ('SNWD', 49575718), ('SX02', 3330), ('SX03', 6), ('SX11', 351), ('SX12', 6673), ('SX13', 1087), ('SX14', 424), ('SX15', 335), ('SX17', 335), ('SX21', 30), ('SX22', 30), ('SX23', 30), ('SX31', 104859), ('SX32', 847573), ('SX33', 78629), ('SX34', 1266), ('SX35', 27647), ('SX36', 14764), ('SX51', 32060), ('SX52', 347628), ('SX53', 41652), ('SX54', 981), ('SX55', 17631), ('SX56', 6635), ('SX57', 62), ('TAVG', 34466932), ('THIC', 12740), ('TMAX', 75902723), ('TMIN', 75618625), ('TOBS', 28647300), ('TSUN', 20921), ('WDF2', 5908670), ('WDF5', 5722615), ('WDFG', 1716700), ('WDMV', 590118), ('WESD', 7692172), ('WESF', 3985175), ('WSF2', 5910048), ('WSF5', 5725333), ('WSFG', 1797107), ('WSFI', 142462), ('WT01', 2553542), ('WT02', 325617), ('WT03', 1019030), ('WT04', 55933), ('WT05', 82236), ('WT06', 71872), ('WT07', 13277), ('WT08', 686932), ('WT09', 19993), ('WT10', 471), ('WT11', 70644), ('WT13', 179659), ('WT14', 10954), ('WT15', 522), ('WT16', 187410), ('WT17', 2776), ('WT18', 54972), ('WT19', 8248), ('WT21', 1752), ('WT22', 12854)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept after QA+status+missing+element filters: 321942526 (54.8%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'US1Hami1858', 'date': '20181024', 'element': 'PRCP', 'value': 0, 'qflag': '', 'year': 2018, 'status': 'valid', 'value_si': 0.0}\n",
      "{'id': 'US1INGR0018', 'date': '20181025', 'element': 'PRCP', 'value': 0, 'qflag': '', 'year': 2018, 'status': 'valid', 'value_si': 0.0}\n",
      "{'id': 'US1WAGH0033', 'date': '20180919', 'element': 'PRCP', 'value': 0, 'qflag': '', 'year': 2018, 'status': 'valid', 'value_si': 0.0}\n",
      "{'id': 'USR0000OWAG', 'date': '20181020', 'element': 'TMAX', 'value': 161, 'qflag': '', 'year': 2018, 'status': 'valid', 'value_si': 16.1}\n",
      "{'id': 'JM000078388', 'date': '20181023', 'element': 'TMIN', 'value': 248, 'qflag': '', 'year': 2018, 'status': 'valid', 'value_si': 24.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "def to_int_safe(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def to_rec(row) -> Dict[str, Any]:\n",
    "    d = row.asDict()\n",
    "    return {\n",
    "        \"id\": d.get(\"station\"),                 \n",
    "        \"date\": d.get(\"date_str\"),              \n",
    "        \"element\": d.get(\"element\"),\n",
    "        \"value\": to_int_safe(d.get(\"raw_value\")), \n",
    "        \"qflag\": (d.get(\"qflag\") or \"\"),        \n",
    "        \"year\": d.get(\"year\"),\n",
    "        \"status\": (d.get(\"_status\") or \"\")      \n",
    "    }\n",
    "\n",
    "# Map to simple dicts\n",
    "rdd1 = rdd_raw.map(to_rec)\n",
    "\n",
    "# Metrics pre-clean (RDD-only)\n",
    "total_in = rdd1.count()\n",
    "elem_counts_pre = (rdd1.map(lambda r: (r[\"element\"], 1)).reduceByKey(lambda a,b: a+b).collect())\n",
    "print(f\"Total rows (bounded): {total_in}\")\n",
    "print(\"Elements present (pre-clean):\", sorted(elem_counts_pre))\n",
    "\n",
    "# Cleaning\n",
    "rdd_clean = (\n",
    "    rdd1\n",
    "    # keep QA-passed\n",
    "    .filter(lambda r: r[\"qflag\"] == \"\")\n",
    "    # keep valid ingestion status if present\n",
    "    .filter(lambda r: (r[\"status\"] in (\"\", \"valid\")))\n",
    "    # drop sentinel/missing\n",
    "    .filter(lambda r: (r[\"value\"] is not None and r[\"value\"] != -9999))\n",
    "    # keep the three core elements\n",
    "    .filter(lambda r: r[\"element\"] in ELEMENTS)\n",
    ")\n",
    "\n",
    "# Deduplicate (id, date, element)\n",
    "rdd_unique = (\n",
    "    rdd_clean\n",
    "    .map(lambda r: ((r[\"id\"], r[\"date\"], r[\"element\"]), r))\n",
    "    .reduceByKey(lambda a, b: a)   # keep first\n",
    "    .map(lambda kv: kv[1])\n",
    ")\n",
    "\n",
    "# Unit conversion (SI): /10 for TMAX/TMIN/PRCP\n",
    "rdd_si = rdd_unique.map(lambda r: {**r, \"value_si\": r[\"value\"] / 10.0})\n",
    "\n",
    "kept = rdd_si.count()\n",
    "print(f\"Kept after QA+status+missing+element filters: {kept} ({kept/total_in*100:.1f}%)\")\n",
    "\n",
    "# Show few examples\n",
    "for rec in rdd_si.take(VERBOSE_SAMPLES):\n",
    "    print(rec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ce07b7",
   "metadata": {},
   "source": [
    "## 05. Combine to daily rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "052a2199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample daily rows:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('USC00227815', '20150416', 2015, 26.1, 16.1, 21.1, 0.5)\n",
      "('USC00392984', '20210429', 2021, 17.2, 3.3, 10.25, 0.0)\n",
      "('RSM00031987', '20230328', 2023, 11.4, -4.3, 3.5500000000000003, 0.0)\n",
      "('USC00408238', '20130704', 2013, 25.6, 18.3, 21.950000000000003, 0.0)\n",
      "('US1NCML0016', '20230329', 2023, None, None, None, 2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:======================================================> (45 + 1) / 46]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily row count: 186418577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Combine to one row per (id, date, year)\n",
    "rdd_by_day = (\n",
    "    rdd_si\n",
    "    .map(lambda r: ((r[\"id\"], r[\"date\"], r[\"year\"]), {r[\"element\"]: r[\"value_si\"]}))\n",
    "    .reduceByKey(lambda a,b: {**a, **b})\n",
    ")\n",
    "\n",
    "def to_daily(kv):\n",
    "    (id_, date, year) = kv[0]\n",
    "    bag = kv[1]\n",
    "    tmax = bag.get(\"TMAX\")\n",
    "    tmin = bag.get(\"TMIN\")\n",
    "    prcp = bag.get(\"PRCP\")\n",
    "    tavg = (tmax + tmin)/2.0 if (tmax is not None and tmin is not None) else None\n",
    "    return (id_, date, year, tmax, tmin, tavg, prcp)\n",
    "\n",
    "rdd_daily = rdd_by_day.map(to_daily)\n",
    "\n",
    "print(\"Sample daily rows:\")\n",
    "for row in rdd_daily.take(VERBOSE_SAMPLES):\n",
    "    print(row)\n",
    "print(\"Daily row count:\", rdd_daily.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c1da6b",
   "metadata": {},
   "source": [
    "## 06. Convert to DataFrame (write only) and Save Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f896371d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 21:25:08 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:08 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:08 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:09 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:09 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:09 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:09 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:09 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:09 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:09 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:10 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:10 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:10 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:10 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:10 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:10 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:10 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:10 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:10 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:10 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:11 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:11 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:11 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:11 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:11 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:11 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:11 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:11 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:11 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:11 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:11 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:11 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:12 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:12 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:12 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:12 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:12 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:12 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:12 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:25:12 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:26:30 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:26:30 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:26:30 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:26:30 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:26:30 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:26:30 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:26:30 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:26:31 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:26:31 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:26:31 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:26:31 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:26:31 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:26:31 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:26:31 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:26:32 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:26:32 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:26:32 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:26:32 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:26:32 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:26:32 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:26:32 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:26:32 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:26:32 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:26:32 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:27:51 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:27:51 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:27:52 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "25/11/26 21:27:52 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver written to: /home/ubuntu/spark-notebooks/project/data/silver\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"id\", T.StringType(), False),\n",
    "    T.StructField(\"yyyymmdd\", T.StringType(), False),\n",
    "    T.StructField(\"year\", T.IntegerType(), True),\n",
    "    T.StructField(\"tmax_c\", T.DoubleType(), True),\n",
    "    T.StructField(\"tmin_c\", T.DoubleType(), True),\n",
    "    T.StructField(\"tavg_c\", T.DoubleType(), True),\n",
    "    T.StructField(\"prcp_mm\", T.DoubleType(), True),\n",
    "])\n",
    "\n",
    "df_silver = spark.createDataFrame(rdd_daily, schema)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Parse date & add convenience fields\n",
    "df_silver = (df_silver\n",
    "    .withColumn(\"date\", F.to_date(F.col(\"yyyymmdd\"), \"yyyyMMdd\"))\n",
    "    .withColumn(\"month\", F.month(\"date\"))\n",
    "    .withColumn(\"day\", F.dayofmonth(\"date\"))\n",
    "    .drop(\"yyyymmdd\")\n",
    ")\n",
    "\n",
    "# Write Silver (Parquet is fine; switch to 'delta' if you prefer)\n",
    "(df_silver\n",
    " .write\n",
    " .mode(\"overwrite\")\n",
    " .partitionBy(\"year\")\n",
    " .format(\"parquet\")\n",
    " .save(SILVER_PATH))\n",
    "\n",
    "print(\"Silver written to:\", SILVER_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83b393",
   "metadata": {},
   "source": [
    "## 07. Validation & sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe7d182b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver partitions (years):\n",
      "+----+\n",
      "|year|\n",
      "+----+\n",
      "|2010|\n",
      "|2011|\n",
      "|2012|\n",
      "|2013|\n",
      "|2014|\n",
      "|2015|\n",
      "|2016|\n",
      "|2017|\n",
      "|2018|\n",
      "|2019|\n",
      "|2020|\n",
      "|2021|\n",
      "|2022|\n",
      "|2023|\n",
      "|2024|\n",
      "|2025|\n",
      "+----+\n",
      "\n",
      "Silver schema:\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- tmax_c: double (nullable = true)\n",
      " |-- tmin_c: double (nullable = true)\n",
      " |-- tavg_c: double (nullable = true)\n",
      " |-- prcp_mm: double (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n",
      "Row count: 186418577\n",
      "Preview:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:======================================================> (34 + 1) / 35]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+------------------+-------+----------+-----+---+----+\n",
      "|id         |tmax_c|tmin_c|tavg_c            |prcp_mm|date      |month|day|year|\n",
      "+-----------+------+------+------------------+-------+----------+-----+---+----+\n",
      "|USR0000MRED|-2.2  |NULL  |NULL              |NULL   |2010-01-01|1    |1  |2010|\n",
      "|USC00360475|NULL  |NULL  |NULL              |0.5    |2010-01-01|1    |1  |2010|\n",
      "|US1NMDA0165|NULL  |NULL  |NULL              |0.0    |2010-01-01|1    |1  |2010|\n",
      "|MXN00001090|21.7  |5.6   |13.649999999999999|7.4    |2010-01-01|1    |1  |2010|\n",
      "|USW00093806|NULL  |NULL  |NULL              |1.5    |2010-01-01|1    |1  |2010|\n",
      "|USC00412772|NULL  |NULL  |NULL              |0.0    |2010-01-01|1    |1  |2010|\n",
      "|US1TXCLL019|NULL  |NULL  |NULL              |0.0    |2010-01-01|1    |1  |2010|\n",
      "|US1COJF0125|NULL  |NULL  |NULL              |0.0    |2010-01-01|1    |1  |2010|\n",
      "|USC00205406|NULL  |NULL  |NULL              |0.5    |2010-01-01|1    |1  |2010|\n",
      "|US1NYMD0009|NULL  |NULL  |NULL              |2.3    |2010-01-01|1    |1  |2010|\n",
      "+-----------+------+------+------------------+-------+----------+-----+---+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read back and verify partitioning & counts\n",
    "df_check = spark.read.parquet(SILVER_PATH)\n",
    "\n",
    "print(\"Silver partitions (years):\")\n",
    "if \"year\" in df_check.columns:\n",
    "    df_check.select(\"year\").distinct().orderBy(\"year\").show(40, truncate=False)\n",
    "\n",
    "print(\"Silver schema:\")\n",
    "df_check.printSchema()\n",
    "\n",
    "print(\"Row count:\", df_check.count())\n",
    "\n",
    "print(\"Preview:\")\n",
    "df_check.orderBy(\"date\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cf121a",
   "metadata": {},
   "source": [
    "## 08. MetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "289b9cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 21:32:08 WARN MemoryManager: Total allocation exceeds 95.00% (996,147,188 bytes) of heap memory\n",
      "Scaling row group sizes to 92.77% for 8 writers\n",
      "[Stage 37:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      "  stations  -> /home/ubuntu/spark-notebooks/project/data/silver_meta/stations.parquet\n",
      "  inventory -> /home/ubuntu/spark-notebooks/project/data/silver_meta/inventory.parquet\n",
      "  coverage  -> /home/ubuntu/spark-notebooks/project/data/silver_meta/coverage.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# === Silver: parse & persist NOAA metadata (stations + inventory) ===\n",
    "import os\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "STATIONS_TXT  = \"/home/ubuntu/spark-notebooks/project/data/meta/ghcnd-stations.txt\"\n",
    "INVENTORY_TXT = \"/home/ubuntu/spark-notebooks/project/data/meta/ghcnd-inventory.txt\"\n",
    "\n",
    "OUT_DIR = \"/home/ubuntu/spark-notebooks/project/data/silver_meta\"\n",
    "STATIONS_PQ  = f\"{OUT_DIR}/stations.parquet\"\n",
    "INVENTORY_PQ = f\"{OUT_DIR}/inventory.parquet\"\n",
    "COVERAGE_PQ  = f\"{OUT_DIR}/coverage.parquet\"\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Parse ghcnd-stations.txt (fixed width) ---\n",
    "stn_schema = T.StructType([\n",
    "    T.StructField(\"id\",    T.StringType(), False),\n",
    "    T.StructField(\"lat\",   T.DoubleType(), True),\n",
    "    T.StructField(\"lon\",   T.DoubleType(), True),\n",
    "    T.StructField(\"elev\",  T.DoubleType(), True),\n",
    "    T.StructField(\"state\", T.StringType(), True),\n",
    "    T.StructField(\"name\",  T.StringType(), True),\n",
    "    T.StructField(\"gsn\",   T.StringType(), True),\n",
    "    T.StructField(\"hcn_crn\", T.StringType(), True),\n",
    "    T.StructField(\"wmo\",   T.StringType(), True),\n",
    "])\n",
    "\n",
    "stn_rdd = (spark.read.text(STATIONS_TXT).rdd\n",
    "    .map(lambda r: r[0])\n",
    "    .filter(lambda s: len(s) >= 85)\n",
    "    .map(lambda s: (\n",
    "        s[0:11].strip(),\n",
    "        float(s[12:20].strip() or \"nan\"),\n",
    "        float(s[21:30].strip() or \"nan\"),\n",
    "        float(s[31:37].strip() or \"nan\"),\n",
    "        s[38:40].strip() or None,\n",
    "        s[41:71].strip() or None,\n",
    "        s[72:75].strip() or None,\n",
    "        s[76:79].strip() or None,\n",
    "        s[80:85].strip() or None,\n",
    "    ))\n",
    ")\n",
    "stations_df_raw = spark.createDataFrame(stn_rdd, stn_schema).dropDuplicates([\"id\"])\n",
    "\n",
    "# Light cleaning: convert NaNs→nulls; clip lat/lon to valid bounds; normalize text\n",
    "stations_df = (stations_df_raw\n",
    "    .withColumn(\"lat\",  F.when(F.isnan(\"lat\"),  None).otherwise(F.col(\"lat\")))\n",
    "    .withColumn(\"lon\",  F.when(F.isnan(\"lon\"),  None).otherwise(F.col(\"lon\")))\n",
    "    .withColumn(\"elev\", F.when(F.isnan(\"elev\"), None).otherwise(F.col(\"elev\")))\n",
    "    .filter((F.col(\"lat\").isNull()) | ((F.col(\"lat\") >= -90) & (F.col(\"lat\") <= 90)))\n",
    "    .filter((F.col(\"lon\").isNull()) | ((F.col(\"lon\") >= -180) & (F.col(\"lon\") <= 180)))\n",
    "    .withColumn(\"state\", F.upper(F.col(\"state\")))\n",
    "    .withColumn(\"name\",  F.trim(F.col(\"name\")))\n",
    ")\n",
    "\n",
    "(stations_df.write.mode(\"overwrite\").parquet(STATIONS_PQ))\n",
    "\n",
    "# --- Parse ghcnd-inventory.txt (fixed width) ---\n",
    "inv_schema = T.StructType([\n",
    "    T.StructField(\"id\",        T.StringType(), False),\n",
    "    T.StructField(\"lat\",       T.DoubleType(), True),\n",
    "    T.StructField(\"lon\",       T.DoubleType(), True),\n",
    "    T.StructField(\"element\",   T.StringType(), False),\n",
    "    T.StructField(\"first_year\",T.IntegerType(), True),\n",
    "    T.StructField(\"last_year\", T.IntegerType(), True),\n",
    "])\n",
    "\n",
    "inv_rdd = (spark.read.text(INVENTORY_TXT).rdd\n",
    "    .map(lambda r: r[0])\n",
    "    .filter(lambda s: len(s) >= 45)\n",
    "    .map(lambda s: (\n",
    "        s[0:11].strip(),\n",
    "        float(s[12:20].strip() or \"nan\"),\n",
    "        float(s[21:30].strip() or \"nan\"),\n",
    "        s[31:35].strip(),\n",
    "        int((s[36:40].strip() or \"0\")),\n",
    "        int((s[41:45].strip() or \"0\")),\n",
    "    ))\n",
    ")\n",
    "inventory_df_raw = spark.createDataFrame(inv_rdd, inv_schema)\n",
    "\n",
    "inventory_df = (inventory_df_raw\n",
    "    .withColumn(\"lat\",  F.when(F.isnan(\"lat\"),  None).otherwise(\"lat\"))\n",
    "    .withColumn(\"lon\",  F.when(F.isnan(\"lon\"),  None).otherwise(\"lon\"))\n",
    "    .filter(F.col(\"element\").isin(\"TMAX\",\"TMIN\",\"PRCP\",\"TAVG\",\"SNOW\",\"SNWD\"))  # keep common ones (adjust if you like)\n",
    ")\n",
    "\n",
    "(inventory_df.write.mode(\"overwrite\").parquet(INVENTORY_PQ))\n",
    "\n",
    "# --- Tiny helper table used often in Gold: coverage per station for your core elements ---\n",
    "coverage_df = (inventory_df\n",
    "    .filter(F.col(\"element\").isin(\"TMAX\",\"TMIN\",\"PRCP\"))\n",
    "    .groupBy(\"id\")\n",
    "    .agg(\n",
    "        F.min(\"first_year\").alias(\"min_first_year\"),\n",
    "        F.max(\"last_year\").alias(\"max_last_year\")\n",
    "    )\n",
    ")\n",
    "\n",
    "(coverage_df.write.mode(\"overwrite\").parquet(COVERAGE_PQ))\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\"  stations  ->\", STATIONS_PQ)\n",
    "print(\"  inventory ->\", INVENTORY_PQ)\n",
    "print(\"  coverage  ->\", COVERAGE_PQ)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
